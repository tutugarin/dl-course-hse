# Отчёт по выполнению БДЗ по курсу DL

- итоговая точность на кагле: `0.64240`

*Всё началось самым обычным осенним утром типичного пмишника: много дедлайнов, много домашек, много контрольных мероприятий. И вот, казалось бы, новый обычный дедлайн: выложили первую БДЗ по ДЛ. Всё начиналось хорошо, спокойно, думаешь, что поставишь обучаться модель, пойдешь деать другие дела. Ага, конечно, как же я ошибался...*

## Глава I: играемся с кастомными моделями

*ох уж этот подбор архитектуры...*

В начале была идея, что можно при помощи только архитектуры достичь точности в `0.5` на валидации. За основу была взята идея прокидывания градиентов (`residual connection`), иначе модель вовсе не обучалась, градиент слишком быстро затухал.

- Изначально я пытался завести блок из `GoogLeNet` под названием `Inception block`:

    ![](https://miro.medium.com/max/1400/1*DKjGRDd_lJeUfVlY50ojOA.png)

    Но я уперся в потолок в примерно `0.44` точности. 

- решил повеселиться, пробовал делать `Inception block` шире, уже, глубже, пробовал разные размеры ядра и разные `stride`. Но все равно потолок не был способен пробить

- после этого решил вернуться к классике и сначала написать обычный `ResNet` (просто настакать побольше блоков). Также пробовал разные глубины, разные функции активации, разные связи между блоками (`AvgPool2d` или `MaxPool2d`). Но всё равно я не смог побить потолок. 

- Всё это сопровождалось попытками подобрать расписание длины шага. Основым моим объектом интереса был [One Cycle lr policy](https://arxiv.org/abs/1708.07120). Но так же пробовал и другие расписания (`ExponentialLR`, `MultiStepLR`, `CyclicLR`)

*тут начали приходить мысли, что проблема может быть не только в архитектуре, но и в самих аугментациях*

- тут пришла идея ресайзить исходное тренировчное изображение в 256 $\times$ 256 (из 64 $\times$ 64), а потом выбирать случайный кадр размера 224 $\times$ 224. (тестовые изображения просто ресайзились в 224 $\times$ 224). Это дало прирост в качестве до `0.53`.

- также попробовал из тестового изображения брать только центральную часть (в недежде, что она содержит основную информацию): добавило к точности еще `+0.02`

- интересной штукой, которую я опробовал, это стохастическая глубина нейронки. То есть какие-то слои с некоторой вероятностью во время обучения могут зануляться. ([статья](https://arxiv.org/pdf/1603.09382.pdf)). Это реализовано было при функции [torchvision.ops.StochasticDepth](https://pytorch.org/vision/stable/generated/torchvision.ops.StochasticDepth.html#torchvision.ops.StochasticDepth). Эта функция хорошо помогала бороться с переобучением.

## Глава II: да пошли эти ваши кастомные архитектуры куда подальше

*мне очень понравилось играться со своими архитектурами, но зачастую непонятно, это ты неписал что-то неработающее или нужно просто докрутить аугментации*

- была выбрана архитектура `ResNet50`. 

- без аугментаций (ну точнее с обычными, по типу различных флипов по осям, небольшими изменениями цвета, изменение угла, под которым смотрится на изображение) точность составила примерно `0.58` на валидации:

    ![](https://sun9-86.userapi.com/impg/ux-iEp58WU2KKD1bnP-Xnr5YIsa0ko5DhhjFmg/IoOoKn-FAmk.jpg?size=2560x1345&quality=95&sign=b42cea5c3bfe2faceaf585b6a24b2b88&type=album)

- Дальше я попробовал вырезать случайный квадрат ([статья](https://arxiv.org/pdf/1708.04896.pdf)). Тут я и пришел к финальному решению, которая выдала наилучший результат в `0.64240`

    ![](https://sun9-79.userapi.com/impg/qbqGwDtEzWLKdJBzSfc-0Jrhe-TWukFIJYugjw/sJHW8pyRrC8.jpg?size=2560x1345&quality=95&sign=212ff5ffc50baa1fd224d775b9c61073&type=album)

- до этого я использовал `CosineAnnealingLR`, под конец решил попробовать `ReduceLROnPlateau`. Получился такой же результат, но с интересным графиком:

    ![](https://sun9-20.userapi.com/impg/YxESzctOzqwVN4Y2sRSt7XVDxgBiT9lsE1skFg/4mpqzC2QCqk.jpg?size=2560x1345&quality=95&sign=875868f2868b3bc5216eb709788a0daf&type=album)

## Глава III: итог

*Жаль, что мы слишком быстро разбазарили ФКН, и нам не хватило денюшек на датасферу. Мне бы хотелось еще поиграться со своими архитектурами и попробовать интересные аугментации на них.*

Итоговые фичи: 

- запускался код в датасферена на `g1.1` 

- `CosineAnnealingLR` c начальным `lr=0.01`

- optimizer: `SGD + momentum=0.9`

- гланые интересные аугментации `RandomErasing` и `RandomResizedCrop`

- weight decay = `3 * 1e-5`

- нормализация кастомная, посчитал отдельно на тренировочных данных `transforms.Normalize([0.5696, 0.5450, 0.4936], [0.2430, 0.2375, 0.2555]),`